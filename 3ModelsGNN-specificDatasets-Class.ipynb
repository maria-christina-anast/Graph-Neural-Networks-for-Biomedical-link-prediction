{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "841d46e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mariachristina/anaconda3/lib/python3.11/site-packages/torch_geometric/typing.py:72: UserWarning: An issue occurred while importing 'torch-scatter'. Disabling its usage. Stacktrace: dlopen(/Users/mariachristina/anaconda3/lib/python3.11/site-packages/torch_scatter/_version_cpu.so, 0x0006): Symbol not found: __ZN3c1017RegisterOperatorsD1Ev\n",
      "  Referenced from: <084F0101-0C02-3262-85FB-B16F3CD9274E> /Users/mariachristina/anaconda3/lib/python3.11/site-packages/torch_scatter/_version_cpu.so\n",
      "  Expected in:     <D400622C-0C6B-3AE1-AB45-F1D0BF19B384> /Users/mariachristina/anaconda3/lib/python3.11/site-packages/torch/lib/libtorch_cpu.dylib\n",
      "  warnings.warn(f\"An issue occurred while importing 'torch-scatter'. \"\n",
      "/Users/mariachristina/anaconda3/lib/python3.11/site-packages/torch_geometric/typing.py:110: UserWarning: An issue occurred while importing 'torch-sparse'. Disabling its usage. Stacktrace: dlopen(/Users/mariachristina/anaconda3/lib/python3.11/site-packages/torch_sparse/_version_cpu.so, 0x0006): Symbol not found: __ZN3c1017RegisterOperatorsD1Ev\n",
      "  Referenced from: <1BBCA6F9-4348-38E7-BE49-97514DC7CE1C> /Users/mariachristina/anaconda3/lib/python3.11/site-packages/torch_sparse/_version_cpu.so\n",
      "  Expected in:     <D400622C-0C6B-3AE1-AB45-F1D0BF19B384> /Users/mariachristina/anaconda3/lib/python3.11/site-packages/torch/lib/libtorch_cpu.dylib\n",
      "  warnings.warn(f\"An issue occurred while importing 'torch-sparse'. \"\n",
      "/Users/mariachristina/anaconda3/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "import rdflib\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, GATConv\n",
    "from torch_geometric.data import Data\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "import re\n",
    "from scholarly import scholarly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "669a6e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphLinkPredictor:\n",
    "    \n",
    "    \"\"\"\n",
    "    A class for performing link prediction on RDF graphs using embeddings and \n",
    "    various machine learning models, specifically graph neural networks (GNNs).\n",
    "\n",
    "    Attributes:\n",
    "    -----------\n",
    "    rdf_files (list): \n",
    "        A list of RDF files to be loaded and processed.\n",
    "    edge_label_mapping (dict): \n",
    "        A mapping of RDF predicates to edge labels used in the network.\n",
    "    embeddings_file (str): \n",
    "        Path to the file containing precomputed node embeddings.\n",
    "    mapping_file (str): \n",
    "        Path to the file containing the mapping from nodes to embedding indices.\n",
    "    graphs (list): \n",
    "        A list to hold the RDFLib Graph objects parsed from the RDF files.\n",
    "    g (rdflib.Graph): \n",
    "        A combined RDFLib Graph containing data from all provided RDF files.\n",
    "    G (networkx.Graph): \n",
    "        A NetworkX graph created from the RDFLib Graph.\n",
    "    embeddings (numpy.ndarray): \n",
    "        Loaded embeddings corresponding to the graph nodes.\n",
    "    node_to_idx (dict): \n",
    "        A mapping from node URIs to their respective embedding indices.\n",
    "    idx_to_node (dict): \n",
    "        A mapping from embedding indices to their respective node URIs.\n",
    "    x (torch.Tensor): \n",
    "        A tensor representation of the node embeddings.\n",
    "    edge_index (torch.Tensor): \n",
    "        A tensor representing edges in the graph.\n",
    "    data (torch_geometric.data.Data): \n",
    "        A PyTorch Geometric Data object containing node features and edges.\n",
    "    train_edges (list): \n",
    "        A list of edges used for training.\n",
    "    test_edges (list): \n",
    "        A list of edges used for testing.\n",
    "    negative_train_edges (list): \n",
    "        A list of non-existent edges used as negative examples during training.\n",
    "    negative_test_edges (list): \n",
    "        A list of non-existent edges used as negative examples during testing.\n",
    "    train_edge_index (torch.Tensor): \n",
    "        A tensor representing the edges used in training.\n",
    "    train_edge_labels (torch.Tensor): \n",
    "        A tensor representing the labels (existence or non-existence) of training edges.\n",
    "    train_losses (list): \n",
    "        A list to store training loss values over epochs.\n",
    "    test_losses (list): \n",
    "        A list to store test loss values over epochs.\n",
    "    model (torch.nn.Module): \n",
    "        The GNN model being trained.\n",
    "    criterion (torch.nn.Module): \n",
    "        The loss function used during training.\n",
    "    optimizer (torch.optim.Optimizer): \n",
    "        The optimizer used to update model parameters during training.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, rdf_files, edge_label_mapping, embeddings_file, mapping_file):\n",
    "        \n",
    "        \"\"\"\n",
    "        Initializes the GraphLinkPredictor with RDF files, edge label mappings, \n",
    "        and paths to the embeddings and mapping files.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        rdf_files (list): \n",
    "            A list of RDF file paths.\n",
    "        edge_label_mapping (dict): \n",
    "            A mapping of RDF predicates to edge labels.\n",
    "        embeddings_file (str): \n",
    "            Path to the node embeddings file.\n",
    "        mapping_file (str): \n",
    "            Path to the node-to-index mapping file.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.rdf_files = rdf_files\n",
    "        self.edge_label_mapping = edge_label_mapping\n",
    "        self.embeddings_file = embeddings_file\n",
    "        self.mapping_file = mapping_file\n",
    "        self.graphs = []\n",
    "        self.load_graphs()\n",
    "        self.create_networkx_graph()\n",
    "        self.load_embeddings_and_mappings()\n",
    "        self.create_tensors()\n",
    "    \n",
    "    def load_graphs(self):\n",
    "        \n",
    "        \"\"\"\n",
    "        Loads RDF files into RDFLib Graph objects and combines them into a single graph.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.graphs = [rdflib.Graph() for _ in self.rdf_files]\n",
    "        for g, rdf_file in zip(self.graphs, self.rdf_files):\n",
    "            g.parse(rdf_file, format=\"xml\")\n",
    "\n",
    "        # Combine all graphs into one\n",
    "        self.g = rdflib.Graph()\n",
    "        for graph in self.graphs:\n",
    "            self.g += graph\n",
    "    \n",
    "    def create_networkx_graph(self):\n",
    "        \n",
    "        \"\"\"\n",
    "        Converts the combined RDFLib Graph into a NetworkX graph, using the specified \n",
    "        edge label mapping to define edges.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.G = nx.Graph()\n",
    "        for subj, pred, obj in self.g:\n",
    "            relation = str(pred)\n",
    "            if relation in self.edge_label_mapping:\n",
    "                self.G.add_edge(str(subj), str(obj), relation=relation)\n",
    "    \n",
    "    def load_embeddings_and_mappings(self):\n",
    "        \n",
    "        \"\"\"\n",
    "        Loads precomputed node embeddings from a file and constructs mappings between \n",
    "        nodes and their corresponding embedding indices.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.embeddings = np.load(self.embeddings_file)\n",
    "        self.node_to_idx = {}\n",
    "        self.idx_to_node = {}\n",
    "        with open(self.mapping_file, 'r') as f:\n",
    "            for line in f:\n",
    "                node, idx = line.strip().split('\\t')\n",
    "                self.node_to_idx[node] = int(idx)\n",
    "                self.idx_to_node[int(idx)] = node\n",
    "        self.x = torch.tensor(self.embeddings, dtype=torch.float)\n",
    "    \n",
    "    def create_tensors(self):\n",
    "        \n",
    "        \"\"\"\n",
    "        Creates PyTorch tensors for graph edges and prepares data for model training.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.edge_index = torch.tensor([[self.node_to_idx[src], self.node_to_idx[dst]] for src, dst in self.G.edges()], dtype=torch.long).t().contiguous()\n",
    "        self.data = Data(x=self.x, edge_index=self.edge_index)\n",
    "    \n",
    "    def load_edge_data(self, files):\n",
    "        \n",
    "        \"\"\"\n",
    "        Loads training and test edges, along with their corresponding negative examples, \n",
    "        from provided files.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        files (dict): \n",
    "            A dictionary containing paths to files with train, test, and negative edges.\n",
    "        \"\"\"\n",
    "        \n",
    "        with open(files['train_edges'], 'rb') as f:\n",
    "            self.train_edges = pickle.load(f)\n",
    "        with open(files['test_edges'], 'rb') as f:\n",
    "            self.test_edges = pickle.load(f)\n",
    "        with open(files['negative_train_edges'], 'rb') as f:\n",
    "            self.negative_train_edges = pickle.load(f)\n",
    "        with open(files['negative_test_edges'], 'rb') as f:\n",
    "            self.negative_test_edges = pickle.load(f)\n",
    "        \n",
    "        self.train_edges = [(u, v) for u, v in self.train_edges if self.G.has_edge(u, v)]\n",
    "        self.test_edges = [(u, v) for u, v in self.test_edges if self.G.has_edge(u, v)]\n",
    "        \n",
    "        self.train_edge_index = torch.tensor([[self.node_to_idx[src], self.node_to_idx[dst]] for src, dst in self.train_edges + self.negative_train_edges], dtype=torch.long).t().contiguous()\n",
    "        self.train_edge_labels = torch.tensor([1] * len(self.train_edges) + [0] * len(self.negative_train_edges), dtype=torch.float)\n",
    "    \n",
    "    def train_model(self, model, criterion, optimizer, epochs=100):\n",
    "        \n",
    "        \"\"\"\n",
    "        Trains the GNN model on the provided graph data using the specified loss function \n",
    "        and optimizer.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        model (torch.nn.Module): \n",
    "            The GNN model to be trained.\n",
    "        criterion (torch.nn.Module): \n",
    "            The loss function used during training.\n",
    "        optimizer (torch.optim.Optimizer): \n",
    "            The optimizer used to update model parameters.\n",
    "        epochs (int): \n",
    "            Number of training epochs (default is 100).\n",
    "        \"\"\"\n",
    "        \n",
    "        self.model = model\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.train_losses = []\n",
    "        self.test_losses = []\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            self.model.train()\n",
    "            self.optimizer.zero_grad()\n",
    "            train_scores = self.model(self.data, self.edge_index, self.train_edge_index)\n",
    "            train_loss = self.criterion(train_scores, self.train_edge_labels)\n",
    "            train_loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            self.train_losses.append(train_loss.item())\n",
    "            \n",
    "            self.model.eval()\n",
    "            with torch.no_grad():\n",
    "                test_edge_index = torch.tensor([[self.node_to_idx[src], self.node_to_idx[dst]] for src, dst in self.test_edges + self.negative_test_edges], dtype=torch.long).t().contiguous()\n",
    "                test_edge_labels = torch.tensor([1] * len(self.test_edges) + [0] * len(self.negative_test_edges), dtype=torch.float)\n",
    "                \n",
    "                test_scores = self.model(self.data, self.edge_index, test_edge_index)\n",
    "                test_loss = self.criterion(test_scores, test_edge_labels)\n",
    "                self.test_losses.append(test_loss.item())\n",
    "                \n",
    "                test_probs = torch.sigmoid(test_scores)\n",
    "                test_preds = (test_probs >= 0.5).float()\n",
    "\n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                print(f'Epoch {epoch+1}, Training Loss: {train_loss.item()}, Test Loss: {test_loss.item()}')\n",
    "    \n",
    "    def evaluate_model(self):\n",
    "        \n",
    "        \"\"\"\n",
    "        Evaluates the trained model on the test data, printing the accuracy, classification \n",
    "        report, and plotting the confusion matrix.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            test_edge_index = torch.tensor([[self.node_to_idx[src], self.node_to_idx[dst]] for src, dst in self.test_edges + self.negative_test_edges], dtype=torch.long).t().contiguous()\n",
    "            test_edge_labels = torch.tensor([1] * len(self.test_edges) + [0] * len(self.negative_test_edges), dtype=torch.float)\n",
    "            test_scores = self.model(self.data, self.edge_index, test_edge_index)\n",
    "            \n",
    "            test_probs = torch.sigmoid(test_scores)\n",
    "            test_preds = (test_probs >= 0.5).float()\n",
    "            \n",
    "            cm = confusion_matrix(test_edge_labels.numpy(), test_preds.numpy())\n",
    "            accuracy = accuracy_score(test_edge_labels.numpy(), test_preds.numpy())\n",
    "            class_report = classification_report(test_edge_labels.numpy(), test_preds.numpy())\n",
    "            \n",
    "            print(f\"Accuracy: {accuracy}\")\n",
    "            print(\"Classification Report:\")\n",
    "            print(class_report)\n",
    "            \n",
    "            plt.figure(figsize=(8, 6))\n",
    "            sns.heatmap(cm, annot=True, fmt=\"d\")\n",
    "            plt.xlabel('Predicted Label')\n",
    "            plt.ylabel('True Label')\n",
    "            plt.title('Confusion Matrix')\n",
    "            plt.show()\n",
    "    \n",
    "    def predict_new_links(self, new_edges):\n",
    "        \n",
    "        \"\"\"\n",
    "        Predicts the existence of new links (edges) in the graph using the trained model.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        new_edges (list): \n",
    "            A list of new edges to predict, where each edge is a tuple of (source_node, target_node).\n",
    "\n",
    "        Returns:\n",
    "        -----------\n",
    "        torch.Tensor: \n",
    "            The predicted scores for the new edges.\n",
    "        \"\"\"\n",
    "        \n",
    "        edge_indices = torch.tensor([[self.node_to_idx[src], self.node_to_idx[dst]] for src, dst in new_edges], dtype=torch.long).t().contiguous()\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            scores = self.model(self.data, self.edge_index, edge_indices)\n",
    "        return scores\n",
    "\n",
    "\n",
    "    \n",
    "    def format_url(self, entity_type, entity_id):\n",
    "        \n",
    "        \"\"\"\n",
    "        Formats a URL for a given entity based on its type and identifier.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        entity_type (str): \n",
    "            The type of the entity (e.g., 'sideeffect').\n",
    "        entity_id (str): \n",
    "            The identifier of the entity.\n",
    "\n",
    "        Returns:\n",
    "        -----------\n",
    "        str: \n",
    "            The formatted URL.\n",
    "        \"\"\"\n",
    "        \n",
    "        if entity_type == 'sideeffect':\n",
    "            return f\"http://erias.fr/oregano/side_effect/{entity_id}\"\n",
    "        else:\n",
    "            return f\"http://erias.fr/oregano/{entity_type}/{entity_id}\"\n",
    "\n",
    "    def read_misclassified_edges(self, file_path):\n",
    "        \n",
    "        \"\"\"\n",
    "        Reads a CSV file containing misclassified edges and formats them for further processing.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        file_path (str): \n",
    "            Path to the CSV file containing misclassified edges.\n",
    "\n",
    "        Returns:\n",
    "        -----------\n",
    "        list: \n",
    "            A list of dictionaries, where each dictionary contains the 'edge', 'actual_label', 'predicted_label', \n",
    "            and 'score'.\n",
    "        \"\"\"\n",
    "        \n",
    "        df = pd.read_csv(file_path)\n",
    "        misclassified_edges = df[df['actual label'] != df['predicted label']]\n",
    "        new_edges = []\n",
    "        for _, row in misclassified_edges.iterrows():\n",
    "            from_url = self.format_url(row['from'].split('_')[0], row['from'])\n",
    "            to_url = self.format_url(row['to'].split('_')[0], row['to'])\n",
    "            new_edges.append({\n",
    "                'edge': (from_url, to_url),\n",
    "                'actual_label': row['actual label'],\n",
    "                'predicted_label': row['predicted label'],\n",
    "                'score': row['score']\n",
    "            })\n",
    "        return new_edges\n",
    "\n",
    "    def create_results_df(self, new_edges, scores_prob):\n",
    "        \n",
    "        \"\"\"\n",
    "        Creates a DataFrame containing the results of predictions for misclassified edges.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        new_edges (list): \n",
    "            A list of misclassified edges formatted as dictionaries.\n",
    "        scores_prob (torch.Tensor): \n",
    "            The predicted probabilities for the edges.\n",
    "\n",
    "        Returns:\n",
    "        -----------\n",
    "        pandas.DataFrame: \n",
    "            A DataFrame containing the prediction results.\n",
    "        \"\"\"\n",
    "        \n",
    "        results_df = pd.DataFrame({\n",
    "            'From': [self.extract_name(item['edge'][0]) for item in new_edges],\n",
    "            'To': [self.extract_name(item['edge'][1]) for item in new_edges],\n",
    "            'Actual Label': [item['actual_label'] for item in new_edges],\n",
    "            'Predicted Label Node2Vec': [item['predicted_label'] for item in new_edges],\n",
    "            'Node2Vec Score': [item['score'] for item in new_edges],\n",
    "            'GNN Score': [score.item() for score in scores_prob]\n",
    "        })\n",
    "        return results_df\n",
    "\n",
    "\n",
    "    def extract_name(self, uri):\n",
    "        return uri.split('/')[-1]  # Extracts the last part of the URI\n",
    "    \n",
    "    def train_and_evaluate_all_models(self, models_config, epochs=100):\n",
    "        \n",
    "        \"\"\"\n",
    "        Trains and evaluates multiple models as specified in the models_config. \n",
    "        Combines the results into a single DataFrame.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        models_config (list): \n",
    "            A list of tuples containing model information in the format (model_name, gnn_model, link_predictor).\n",
    "        epochs (int): \n",
    "            Number of training epochs for each model (default is 100).\n",
    "\n",
    "        Returns:\n",
    "        -----------\n",
    "        pandas.DataFrame: \n",
    "            A DataFrame containing the combined evaluation results of all models.\n",
    "        \"\"\"\n",
    "        \n",
    "        results = []\n",
    "\n",
    "        for model_name, gnn, link_predictor in models_config:\n",
    "            print(f\"Training and evaluating {model_name}...\")\n",
    "\n",
    "            # Initialize criterion and optimizer for this model\n",
    "            criterion = nn.BCEWithLogitsLoss()\n",
    "            optimizer = torch.optim.Adam(link_predictor.parameters(), lr=0.01)\n",
    "\n",
    "            # Set the model, criterion, and optimizer\n",
    "            self.model = link_predictor\n",
    "            self.criterion = criterion\n",
    "            self.optimizer = optimizer\n",
    "\n",
    "            # Train the model\n",
    "            self.train_model(self.model, self.criterion, self.optimizer, epochs)\n",
    "\n",
    "            # Evaluate the model\n",
    "            self.evaluate_model()\n",
    "\n",
    "            # Predict new links and create results DataFrame\n",
    "            csv_file_path = 'predicted_edges_bestParams.csv'\n",
    "            misclassified_edges = self.read_misclassified_edges(csv_file_path)\n",
    "            scores_prob_all = self.predict_new_links([item['edge'] for item in misclassified_edges])\n",
    "            scores_prob_all = torch.sigmoid(scores_prob_all)\n",
    "\n",
    "            # Create results DataFrame for this model\n",
    "            results_df = self.create_results_df(misclassified_edges, scores_prob_all)\n",
    "            results_df['Model'] = model_name\n",
    "            results_df['Score'] = scores_prob_all.numpy()  # Add model scores as a separate column\n",
    "            results.append(results_df)\n",
    "\n",
    "        # Combine all results into a single DataFrame\n",
    "        sum_df = pd.concat(results, ignore_index=True)\n",
    "\n",
    "        # Pivot the DataFrame to have a separate column for each model's scores\n",
    "        sum_df_pivot = sum_df.pivot_table(index=['From', 'To', 'Actual Label'], columns='Model', values='Score', aggfunc='mean').reset_index()\n",
    "\n",
    "        return sum_df_pivot\n",
    "    \n",
    "    def print_misclassified_edges(self):\n",
    "        \n",
    "        \"\"\"\n",
    "        Identifies and returns misclassified edges from the test set, along with their \n",
    "        actual and predicted labels and prediction scores.\n",
    "\n",
    "        Returns:\n",
    "        -----------\n",
    "        pandas.DataFrame: \n",
    "            A DataFrame containing the misclassified edges and their details.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            test_edge_index = torch.tensor([[self.node_to_idx[src], self.node_to_idx[dst]] for src, dst in self.test_edges + self.negative_test_edges], dtype=torch.long).t().contiguous()\n",
    "            test_edge_labels = torch.tensor([1] * len(self.test_edges) + [0] * len(self.negative_test_edges), dtype=torch.float)\n",
    "            test_scores = self.model(self.data, self.edge_index, test_edge_index)\n",
    "\n",
    "            test_probs = torch.sigmoid(test_scores)\n",
    "            test_preds = (test_probs >= 0.5).float()\n",
    "\n",
    "            misclassified_indices = (test_preds != test_edge_labels).nonzero(as_tuple=True)[0]\n",
    "            misclassified_edges = [self.test_edges[i] if i < len(self.test_edges) else self.negative_test_edges[i - len(self.test_edges)] for i in misclassified_indices.tolist()]\n",
    "            misclassified_labels = test_edge_labels[misclassified_indices].tolist()\n",
    "            misclassified_preds = test_preds[misclassified_indices].tolist()\n",
    "            misclassified_scores = test_probs[misclassified_indices].tolist()\n",
    "\n",
    "            # Extract node names\n",
    "            misclassified_from = [self.extract_name(edge[0]) for edge in misclassified_edges]\n",
    "            misclassified_to = [self.extract_name(edge[1]) for edge in misclassified_edges]\n",
    "\n",
    "            # Create a DataFrame\n",
    "            df = pd.DataFrame({\n",
    "                'From': misclassified_from,\n",
    "                'To': misclassified_to,\n",
    "                'Actual Label': misclassified_labels,\n",
    "                'Predicted Label': misclassified_preds,\n",
    "                'Score': misclassified_scores\n",
    "            })\n",
    "\n",
    "#             print(df)\n",
    "            return df\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c67d56c",
   "metadata": {},
   "source": [
    "# GNN1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3fa02a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the GNN model\n",
    "class GNN1(nn.Module):\n",
    "    \n",
    "    \"\"\"\n",
    "    A Graph Neural Network (GNN) model using Graph Convolutional Network (GCN) layers \n",
    "    for processing graph-structured data.\n",
    "\n",
    "    This model consists of two GCN layers. The first layer transforms the input features \n",
    "    into a 16-dimensional space, and the second layer reduces it further to an 8-dimensional space.\n",
    "\n",
    "    Attributes:\n",
    "        conv1 (GCNConv): The first GCN layer that maps input features to 16 dimensions.\n",
    "        conv2 (GCNConv): The second GCN layer that maps the intermediate features to 8 dimensions.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim):\n",
    "        \n",
    "        \"\"\"\n",
    "        Initializes the GNN1 model with the specified input dimension.\n",
    "\n",
    "        Args:\n",
    "            input_dim (int): The number of input features per node in the graph.\n",
    "        \"\"\"\n",
    "        \n",
    "        super(GNN1, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, 16)\n",
    "        self.conv2 = GCNConv(16, 8)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        \n",
    "        \"\"\"\n",
    "        Defines the forward pass of the GNN1 model.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The node feature matrix with shape [num_nodes, input_dim].\n",
    "            edge_index (torch.Tensor): The edge index tensor with shape [2, num_edges] \n",
    "                                       specifying the graph connectivity.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The output node features after applying the GCN layers.\n",
    "        \"\"\"\n",
    "        \n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "# Define the LinkPredictor model\n",
    "class LinkPredictorGNN1(nn.Module):\n",
    "    \n",
    "    \"\"\"\n",
    "    A link prediction model that utilizes the GNN1 model to predict the existence of links (edges) \n",
    "    in a graph.\n",
    "\n",
    "    This model takes as input the embeddings produced by the GNN1 model for each node, \n",
    "    concatenates the embeddings of the nodes at both ends of an edge, and passes them \n",
    "    through a linear layer to predict the probability of the edge existing.\n",
    "\n",
    "    Attributes:\n",
    "        gnn (GNN1): The GNN model used to generate node embeddings.\n",
    "        linear (nn.Linear): A linear layer that takes concatenated node embeddings and \n",
    "                            outputs a scalar score representing the probability of an edge.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, gnn):\n",
    "        \n",
    "        \"\"\"\n",
    "        Initializes the LinkPredictorGNN1 model with a given GNN model.\n",
    "\n",
    "        Args:\n",
    "            gnn (GNN1): The GNN model that generates node embeddings.\n",
    "        \"\"\"\n",
    "        \n",
    "        super(LinkPredictorGNN1, self).__init__()\n",
    "        self.gnn = gnn\n",
    "        self.linear = nn.Linear(8 * 2, 1)  # 8 from each node's embedding\n",
    "\n",
    "    def forward(self, data, edge_index, edge_label_index):\n",
    "        \n",
    "        \"\"\"\n",
    "        Defines the forward pass of the LinkPredictorGNN1 model.\n",
    "\n",
    "        Args:\n",
    "            data (torch_geometric.data.Data): The PyTorch Geometric data object containing \n",
    "                                              node features and other graph data.\n",
    "            edge_index (torch.Tensor): The edge index tensor with shape [2, num_edges] \n",
    "                                       specifying the graph connectivity.\n",
    "            edge_label_index (torch.Tensor): A tensor with shape [2, num_edges] specifying \n",
    "                                             the indices of the edges to predict.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: A tensor of scores with shape [num_edges], where each score represents \n",
    "                          the predicted probability of the corresponding edge in edge_label_index.\n",
    "        \"\"\"\n",
    "        \n",
    "        x = self.gnn(data.x, edge_index)\n",
    "        edge_embeddings = torch.cat([x[edge_label_index[0]], x[edge_label_index[1]]], dim=-1)\n",
    "        scores = self.linear(edge_embeddings).squeeze()\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927893fd",
   "metadata": {},
   "source": [
    "# GAT1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd9a82c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GAT1(nn.Module):\n",
    "    \n",
    "    \"\"\"\n",
    "    A Graph Attention Network (GAT) model for processing graph-structured data.\n",
    "\n",
    "    This model uses two layers of Graph Attention Convolution (GATConv), where the first layer \n",
    "    has multiple attention heads to capture different aspects of the node neighborhood, \n",
    "    and the second layer reduces the dimensionality of the node features.\n",
    "\n",
    "    Attributes:\n",
    "        conv1 (GATConv): The first GAT layer with 4 attention heads, mapping input features \n",
    "                         to a 16-dimensional space per head (total 64 dimensions).\n",
    "        conv2 (GATConv): The second GAT layer with 1 attention head, reducing the feature \n",
    "                         space to 8 dimensions.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    def __init__(self, input_dim):       \n",
    "        super(GAT1, self).__init__()\n",
    "        self.conv1 = GATConv(input_dim, 16, heads=4)  # 4 attention heads\n",
    "        self.conv2 = GATConv(16 * 4, 8, heads=1)  # 1 attention head for the last layer\n",
    "\n",
    "    def forward(self, x, edge_index):     \n",
    "        x = F.elu(self.conv1(x, edge_index))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "    \n",
    "class LinkPredictorGAT1(nn.Module):\n",
    "    def __init__(self, gnn):\n",
    "        super(LinkPredictorGAT1, self).__init__()\n",
    "        self.gnn = gnn\n",
    "        self.linear = nn.Linear(8 * 2, 1)  # Embedding size from GAT\n",
    "\n",
    "    def forward(self, data, edge_index, edge_label_index):\n",
    "        x = self.gnn(data.x, edge_index)\n",
    "        edge_embeddings = torch.cat([x[edge_label_index[0]], x[edge_label_index[1]]], dim=-1)\n",
    "        scores = self.linear(edge_embeddings).squeeze()\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348b0113",
   "metadata": {},
   "source": [
    "# GAT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72d12e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GATConv\n",
    "\n",
    "class GAT2(nn.Module):\n",
    "    \n",
    "    \"\"\"\n",
    "    A deeper Graph Attention Network (GAT) model for processing graph-structured data.\n",
    "\n",
    "    This model consists of three GAT convolutional layers, each with multiple attention heads,\n",
    "    designed to capture more complex relationships in the graph data.\n",
    "\n",
    "    Attributes:\n",
    "        conv1 (GATConv): The first GAT layer with 4 attention heads, each producing a 32-dimensional output.\n",
    "        conv2 (GATConv): The second GAT layer with 4 attention heads, each producing a 32-dimensional output.\n",
    "        conv3 (GATConv): The third GAT layer with 1 attention head, producing an 8-dimensional output.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    def __init__(self, input_dim):\n",
    "        super(GAT2, self).__init__()\n",
    "        self.conv1 = GATConv(input_dim, 32, heads=4, concat=True)\n",
    "        self.conv2 = GATConv(32 * 4, 32, heads=4, concat=True)\n",
    "        self.conv3 = GATConv(32 * 4, 8, heads=1, concat=True)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.elu(self.conv1(x, edge_index))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = F.elu(self.conv2(x, edge_index))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv3(x, edge_index)\n",
    "        return x\n",
    "\n",
    "class LinkPredictorGAT2(nn.Module):\n",
    "    def __init__(self, gnn):\n",
    "        super(LinkPredictorGAT2, self).__init__()\n",
    "        self.gnn = gnn\n",
    "        self.linear = nn.Linear(8 * 2, 1)\n",
    "\n",
    "    def forward(self, data, edge_index, edge_label_index):\n",
    "        x = self.gnn(data.x, edge_index)\n",
    "        edge_embeddings = torch.cat([x[edge_label_index[0]], x[edge_label_index[1]]], dim=-1)\n",
    "        scores = self.linear(edge_embeddings).squeeze()\n",
    "        return scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20cd83a5",
   "metadata": {},
   "source": [
    "# ComplexGCN\n",
    "\n",
    "Applies **batch normalization** over a batch of features as described in the `\"Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift\" <https://arxiv.org/abs/1502.03167>` paper.\n",
    "\n",
    "\n",
    "The mean and standard-deviation are calculated per-dimension over all nodes inside the mini-batch.\n",
    "\n",
    "**Residual connection** allow the output of a layer to be added directly to the input of a later layer helping gradients flow through very deep networks and making it easier to train deep architectures by addressing issues like vanishing gradients.\n",
    "https://pubmed.ncbi.nlm.nih.gov/37327757/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "224d7ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComplexGCN(nn.Module):\n",
    "    \n",
    "    \"\"\"\n",
    "    A Graph Convolutional Network (GCN) model with residual connections and batch normalization.\n",
    "\n",
    "    This model consists of three GCN layers, each followed by batch normalization, and \n",
    "    incorporates residual connections to improve gradient flow and model performance.\n",
    "\n",
    "    Attributes:\n",
    "        conv1 (GCNConv): The first GCN layer that transforms input features to a 32-dimensional space.\n",
    "        bn1 (nn.BatchNorm1d): Batch normalization layer applied after the first GCN layer.\n",
    "        conv2 (GCNConv): The second GCN layer that further processes the 32-dimensional features.\n",
    "        bn2 (nn.BatchNorm1d): Batch normalization layer applied after the second GCN layer.\n",
    "        conv3 (GCNConv): The third GCN layer that further processes the 32-dimensional features.\n",
    "        bn3 (nn.BatchNorm1d): Batch normalization layer applied after the third GCN layer.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    def __init__(self, input_dim):\n",
    "        super(ComplexGCN, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, 32)\n",
    "        self.bn1 = nn.BatchNorm1d(32)\n",
    "        self.conv2 = GCNConv(32, 32) \n",
    "        self.bn2 = nn.BatchNorm1d(32)\n",
    "        self.conv3 = GCNConv(32, 32) \n",
    "        self.bn3 = nn.BatchNorm1d(32)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # Layer 1\n",
    "        x1 = F.relu(self.bn1(self.conv1(x, edge_index)))\n",
    "        # Layer 2 with residual connection\n",
    "        x2 = F.relu(self.bn2(self.conv2(x1, edge_index)) + x1)\n",
    "        # Layer 3 with residual connection\n",
    "        x3 = self.bn3(self.conv3(x2, edge_index)) + x2 \n",
    "        return x3\n",
    "    \n",
    "\n",
    "class LinkPredictorComplexGCN(nn.Module):\n",
    "    def __init__(self, gnn):\n",
    "        super(LinkPredictorComplexGCN, self).__init__()\n",
    "        self.gnn = gnn\n",
    "        self.linear = nn.Linear(32 * 2, 1) \n",
    "\n",
    "    def forward(self, data, edge_index, edge_label_index):\n",
    "        x = self.gnn(data.x, edge_index)\n",
    "        edge_embeddings = torch.cat([x[edge_label_index[0]], x[edge_label_index[1]]], dim=-1)\n",
    "        scores = self.linear(edge_embeddings).squeeze()\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ea32c6",
   "metadata": {},
   "source": [
    "# Initialize parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91d89a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdf_files = [\n",
    "    \"alzheimer_direct_neighborhood.rdf\",\n",
    "    \"depression_direct_neighborhood_only.rdf\",\n",
    "    \"urinary_direct_neighborhood_only.rdf\",\n",
    "    \"brexpiprazole_direct_neighborhood_only.rdf\",\n",
    "    \"cardiovascular_direct_neighborhood.rdf\",\n",
    "    \"breast_direct_neighborhood.rdf\",\n",
    "    \"ovarian_direct_neighborhood_only.rdf\"\n",
    "]\n",
    "\n",
    "edge_label_mapping = {\n",
    "    'http://erias.fr/oregano/#has_effect',\n",
    "    'http://erias.fr/oregano/#increase_effect',\n",
    "    'http://erias.fr/oregano/#decrease_effect',\n",
    "    'http://erias.fr/oregano/#increase_efficacy',\n",
    "    'http://erias.fr/oregano/#decreases_efficacy',\n",
    "    'http://erias.fr/oregano/#has_indication',\n",
    "    'http://erias.fr/oregano/#has_activity',\n",
    "    'http://erias.fr/oregano/#increase_activity',\n",
    "    'http://erias.fr/oregano/#decrease_activity',\n",
    "    'http://erias.fr/oregano/#has_side_effect',\n",
    "    'http://erias.fr/oregano/#has_target',\n",
    "    'http://erias.fr/oregano/#is_affecting',\n",
    "    'http://erias.fr/oregano/#is_substance_that_treats',\n",
    "    'http://erias.fr/oregano/#acts_within',\n",
    "    'http://erias.fr/oregano/#causes_condition',\n",
    "    'http://erias.fr/oregano/#gene_product_of',\n",
    "    'http://erias.fr/oregano/#has_phenotype'\n",
    "}\n",
    "\n",
    "files = {\n",
    "    'train_edges': \"train_edges.pkl\",\n",
    "    'test_edges': \"test_edges.pkl\",\n",
    "    'negative_train_edges': \"negative_train_edges.pkl\",\n",
    "    'negative_test_edges': \"negative_test_edges.pkl\"\n",
    "}\n",
    "\n",
    "embeddings_file = 'node2vec_embeddings_bestParams.npy'\n",
    "mapping_file = 'node_to_idx_bestParams.txt'\n",
    "\n",
    "predictor = GraphLinkPredictor(rdf_files, edge_label_mapping, embeddings_file, mapping_file)\n",
    "predictor.load_edge_data(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ffd1844",
   "metadata": {},
   "source": [
    "# Run each model separately and investigate the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5be4cdf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gnn = GAT1(input_dim=predictor.x.size(1))\n",
    "# link_predictor = LinkPredictorGAT1(gnn)\n",
    "# criterion = nn.BCEWithLogitsLoss()\n",
    "# optimizer = torch.optim.Adam(link_predictor.parameters(), lr=0.01)\n",
    "\n",
    "# predictor.train_model(link_predictor, criterion, optimizer)\n",
    "# predictor.evaluate_model()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c952d237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Find the misclassified examples and filter\n",
    "# misclassified_df = predictor.print_misclassified_edges()\n",
    "\n",
    "# filtered_df = misclassified_df[(misclassified_df['Actual Label'] == 0)]\n",
    "              \n",
    "\n",
    "# # Condition 1: From is disease and To is gene or compound\n",
    "# condition_1 = filtered_df['From'].str.startswith('disease_') & (filtered_df['To'].str.startswith('gene_') | \\\n",
    "#                                                                 filtered_df['To'].str.startswith('compound_'))\n",
    "\n",
    "# # Condition 2: To is disease and From is gene or compound\n",
    "# condition_2 = filtered_df['To'].str.startswith('disease_') & (filtered_df['From'].str.startswith('gene_') | \\\n",
    "#                                                               filtered_df['From'].str.startswith('compound_'))\n",
    "\n",
    "# # Combine conditions\n",
    "# filtered_df = filtered_df[condition_1 | condition_2]\n",
    "# filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "88ddc3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # GNN1 Model\n",
    "# gnn1 = GNN1(input_dim=predictor.x.size(1))\n",
    "# link_predictor_gnn1 = LinkPredictorGNN1(gnn1)\n",
    "# criterion = nn.BCEWithLogitsLoss()\n",
    "# optimizer = torch.optim.Adam(link_predictor_gnn1.parameters(), lr=0.01)\n",
    "\n",
    "# # Find the misclassified examples for GNN1\n",
    "# misclassified_df_gnn1 = predictor.print_misclassified_edges()\n",
    "\n",
    "# # Filter misclassified edges based on your conditions (GNN1)\n",
    "# filtered_df_gnn1 = misclassified_df_gnn1[(misclassified_df_gnn1['Actual Label'] == 0)]\n",
    "\n",
    "# condition_1_gnn1 = filtered_df_gnn1['From'].str.startswith('disease_') & (filtered_df_gnn1['To'].str.startswith('gene_') | \\\n",
    "#                                                                           filtered_df_gnn1['To'].str.startswith('compound_'))\n",
    "\n",
    "# condition_2_gnn1 = filtered_df_gnn1['To'].str.startswith('disease_') & (filtered_df_gnn1['From'].str.startswith('gene_') | \\\n",
    "#                                                                         filtered_df_gnn1['From'].str.startswith('compound_'))\n",
    "\n",
    "# filtered_df_gnn1 = filtered_df_gnn1[condition_1_gnn1 | condition_2_gnn1]\n",
    "\n",
    "# # Train and evaluate GNN1\n",
    "# predictor.train_model(link_predictor_gnn1, criterion, optimizer)\n",
    "# predictor.evaluate_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "64552945",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Find common misclassified edges between GNN1 and GAT1\n",
    "# common_misclassified = pd.merge(filtered_df_gnn1, filtered_df, on=['From', 'To', 'Actual Label'], how='inner')\n",
    "\n",
    "# # Display the common misclassified edges\n",
    "# common_misclassified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ef3009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example of predicting new links and handling misclassified edges\n",
    "# new_edges = [\n",
    "#     ('http://erias.fr/oregano/compound/compound_8067', 'http://erias.fr/oregano/disease/disease_145'),\n",
    "#     ('http://erias.fr/oregano/disease/disease_3310', 'http://erias.fr/oregano/disease/disease_145'), \n",
    "#     ('http://erias.fr/oregano/disease/disease_804', 'http://erias.fr/oregano/disease/disease_145'),\n",
    "# ]\n",
    "\n",
    "# scores_prob = predictor.predict_new_links(new_edges)\n",
    "# scores_prob = torch.sigmoid(scores_prob)\n",
    "\n",
    "# for edge, score in zip(new_edges, scores_prob):\n",
    "#     print(f\"Link: ({predictor.extract_name(edge[0])}, {predictor.extract_name(edge[1])}): Score = {score.item():.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d810e3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv_file_path = 'predicted_edges_bestParams.csv'\n",
    "# misclassified_edges = predictor.read_misclassified_edges(csv_file_path)\n",
    "# scores_prob_all = predictor.predict_new_links([item['edge'] for item in misclassified_edges])\n",
    "# scores_prob_all = torch.sigmoid(scores_prob_all)\n",
    "\n",
    "# results_df = predictor.create_results_df(misclassified_edges, scores_prob_all)\n",
    "# results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01613d9f",
   "metadata": {},
   "source": [
    "# Run all model and accumulate the results for the misclassified examples of the Node2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09732af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the configuration for each model\n",
    "models_config = [\n",
    "    (\"GNN1\", GNN1(input_dim=64), LinkPredictorGNN1(GNN1(input_dim=64))),\n",
    "    (\"GAT1\", GAT1(input_dim=64), LinkPredictorGAT1(GAT1(input_dim=64))),\n",
    "    (\"GAT2\", GAT2(input_dim=64), LinkPredictorGAT2(GAT2(input_dim=64))),\n",
    "    (\"ComplexGCN\", ComplexGCN(input_dim=64), LinkPredictorComplexGCN(ComplexGCN(input_dim=64)))\n",
    "]\n",
    "\n",
    "# Train and evaluate all models\n",
    "sum_df = predictor.train_and_evaluate_all_models(models_config, epochs=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c85fdb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61667bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract categories from 'From' and 'To' columns\n",
    "sum_df['From_Category'] = sum_df['From'].str.split('_').str[0]\n",
    "sum_df['To_Category'] = sum_df['To'].str.split('_').str[0]\n",
    "\n",
    "# Combine the categories\n",
    "sum_df_melted = sum_df.melt(id_vars=['Actual Label'], value_vars=['From_Category', 'To_Category'], var_name='Type', value_name='Category')\n",
    "\n",
    "# Count occurrences of each category by actual label\n",
    "category_counts = sum_df_melted.groupby(['Actual Label', 'Category']).size().unstack(fill_value=0)\n",
    "\n",
    "palette = sns.color_palette(\"Set2\") \n",
    "\n",
    "\n",
    "# Plotting with the custom palette\n",
    "category_counts.plot(kind='bar', stacked=True, color=palette)\n",
    "plt.title('Number of Links by Category and Actual Label')\n",
    "plt.xlabel('Category')\n",
    "plt.ylabel('Count of Links')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(title='Actual Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441a4198",
   "metadata": {},
   "source": [
    "**Heatmap to identify which models might be redundant (high correlation) or complementary (low correlation).**\n",
    "\n",
    "This information is useful when deciding which models to ensemble or focus on.\n",
    "A strong correlation between certain models, might need further investigation on why they are similar and whether they need diversification to improve overall performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e0ba5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the model columns\n",
    "model_columns = ['ComplexGCN', 'GAT1', 'GAT2', 'GNN1']\n",
    "\n",
    "# Plot heatmap of the predictions\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(sum_df[model_columns].corr(), annot=True, cmap='viridis')\n",
    "plt.title('Correlation Heatmap of Model Predictions')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4949870",
   "metadata": {},
   "source": [
    "**Boxplots to see how well the models distinguish between different classes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f29c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Melt the DataFrame to make it easier to plot\n",
    "melted_df = sum_df.melt(id_vars=['Actual Label'], value_vars=model_columns, \n",
    "                        var_name='Model', value_name='Prediction')\n",
    "\n",
    "# Plot boxplot\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(x='Model', y='Prediction', hue='Actual Label', data=melted_df, palette='viridis')\n",
    "plt.axhline(0.5, color='red', linestyle='--')\n",
    "plt.title('Boxplot of Model Predictions by Actual Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96084eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "for model in model_columns:\n",
    "    fpr, tpr, _ = roc_curve(sum_df['Actual Label'], sum_df[model])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, lw=2, label=f'{model} (area = {roc_auc:.2f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curves for Different Models')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c617ef",
   "metadata": {},
   "source": [
    "**Precision**\n",
    "\n",
    "\"Of all the instances that the model predicted as positive, how many were actually positive?\"\n",
    "\n",
    "**Racall**\n",
    "\n",
    "\"Of all the actual positives, how many did the model successfully identify?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945e5c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "metrics = {}\n",
    "for model in model_columns:\n",
    "    metrics[model] = {\n",
    "        'Accuracy': accuracy_score(sum_df['Actual Label'], (sum_df[model] > 0.5).astype(int)),\n",
    "        'Precision': precision_score(sum_df['Actual Label'], (sum_df[model] > 0.5).astype(int)),\n",
    "        'Recall': recall_score(sum_df['Actual Label'], (sum_df[model] > 0.5).astype(int)),\n",
    "        'F1 Score': f1_score(sum_df['Actual Label'], (sum_df[model] > 0.5).astype(int))\n",
    "    }\n",
    "\n",
    "# Convert to DataFrame\n",
    "metrics_df = pd.DataFrame(metrics).T\n",
    "\n",
    "# Plot bar chart for each metric\n",
    "metrics_df.plot(kind='bar', figsize=(12, 8), colormap='viridis')\n",
    "plt.title('Model Performance Metrics')\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Score')\n",
    "plt.legend(title='Metric')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3d2d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter misclassified rows where Actual Label is 0 and all score columns are > 0.5\n",
    "filtered_df = sum_df[(sum_df['Actual Label'] == 0) & \n",
    "                 (sum_df[['ComplexGCN', 'GAT1', 'GAT2', 'GNN1']].gt(0.5).all(axis=1))]\n",
    "\n",
    "# filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce14ec18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the links disease-compound, disease-gene\n",
    "\n",
    "# Disease-compound links\n",
    "condition1 = ((filtered_df['From_Category'] == 'compound') & (filtered_df['To_Category'] == 'disease')) | \\\n",
    "             ((filtered_df['From_Category'] == 'disease') & (filtered_df['To_Category'] == 'compound'))\n",
    "# Disease-gene links \n",
    "condition2 = ((filtered_df['From_Category'] == 'disease') & (filtered_df['To_Category'] == 'gene')) | \\\n",
    "             ((filtered_df['From_Category'] == 'gene') & (filtered_df['To_Category'] == 'disease'))\n",
    "\n",
    "filtered_df = filtered_df[condition1 | condition2]\n",
    "\n",
    "filtered_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57bfd9c6",
   "metadata": {},
   "source": [
    "# Sparql query to find the labels of the identified entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f80be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install SPARQLWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1df94f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize SPARQL endpoint (replace with your actual endpoint URL)\n",
    "# For this step, it is necessary to have GraphDB for Desktop and load the oreganov2.1_metadata_complete.ttl file with name Graph-1.\n",
    "sparql = SPARQLWrapper(\"http://localhost:7200/repositories/Graph-1\")\n",
    "\n",
    "def get_labels(entity_uri):\n",
    "    query = f\"\"\"\n",
    "    PREFIX oregano: <http://erias.fr/oregano/#>\n",
    "    PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema/#>\n",
    "    SELECT DISTINCT ?label\n",
    "    WHERE {{\n",
    "      <{entity_uri}> rdfs:label ?label.\n",
    "    }}\n",
    "    \"\"\"\n",
    "    \n",
    "    sparql.setQuery(query)\n",
    "    sparql.setReturnFormat(JSON)\n",
    "    try:\n",
    "        results = sparql.query().convert()\n",
    "        labels = [result[\"label\"][\"value\"] for result in results[\"results\"][\"bindings\"]]\n",
    "        return labels\n",
    "    except Exception as e:\n",
    "        print(f\"Query failed for {entity_uri}: {e}\")\n",
    "        return []\n",
    "\n",
    "def construct_uri(entity_id, category):\n",
    "    return f\"http://erias.fr/oregano/{category}/{entity_id}\"\n",
    "\n",
    "# def get_first_label(entity_uri):\n",
    "#     labels = get_labels(entity_uri)\n",
    "#     return labels[0] if labels else None\n",
    "\n",
    "def get_all_labels(entity_uri):\n",
    "    labels = get_labels(entity_uri)\n",
    "    return ', '.join(labels) if labels else None\n",
    "    \n",
    "    \n",
    "# Print all labels\n",
    "filtered_df['From_All_Labels'] = filtered_df.apply(lambda row: get_all_labels(construct_uri(row['From'], row['From_Category'])), axis=1)\n",
    "filtered_df['To_All_Labels'] = filtered_df.apply(lambda row: get_all_labels(construct_uri(row['To'], row['To_Category'])), axis=1)\n",
    "\n",
    "filtered_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b19f223",
   "metadata": {},
   "source": [
    "# Search on web for papers that include both entities of each row by using scholarly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df7797d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install scholarly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54bb6aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to clean labels by removing any prefix before the colon\n",
    "def clean_label(label):\n",
    "    if label:\n",
    "        # Remove any prefix before the colon and strip any extra spaces\n",
    "        cleaned_label = re.sub(r'^[^:]+:', '', label).strip()\n",
    "        return cleaned_label\n",
    "    return None\n",
    "\n",
    "# Function to extract the second label from a comma-separated string\n",
    "def get_second_label(labels_str):\n",
    "    labels = labels_str.split(', ')\n",
    "    return labels[1] if len(labels) > 1 else None\n",
    "\n",
    "# Apply the function to extract the second label\n",
    "filtered_df['From_Second_Label'] = filtered_df['From_All_Labels'].apply(get_second_label)\n",
    "filtered_df['To_Second_Label'] = filtered_df['To_All_Labels'].apply(get_second_label)\n",
    "\n",
    "# Clean the second labels\n",
    "filtered_df['From_Cleaned_Label'] = filtered_df['From_Second_Label'].apply(clean_label)\n",
    "filtered_df['To_Cleaned_Label'] = filtered_df['To_Second_Label'].apply(clean_label)\n",
    "\n",
    "# Display the updated DataFrame\n",
    "# print(filtered_df[['From', 'To', 'From_Cleaned_Label', 'To_Cleaned_Label']])\n",
    "\n",
    "# Function to search for papers using combined labels\n",
    "def search_papers(query):\n",
    "    search_query = scholarly.search_pubs(query)\n",
    "    papers = []\n",
    "    for i in range(5):  # Limit the number of papers retrieved\n",
    "        try:\n",
    "            paper = next(search_query)\n",
    "            papers.append({\n",
    "                'title': paper.get('bib', {}).get('title', 'No title'),\n",
    "#                 'url': paper.get('url', 'No URL')\n",
    "            })\n",
    "        except StopIteration:\n",
    "            break\n",
    "    return papers\n",
    "\n",
    "# Search for papers related to the combination of the two cleaned labels\n",
    "for index, row in filtered_df.iterrows():\n",
    "    from_label = row['From_Cleaned_Label']\n",
    "    to_label = row['To_Cleaned_Label']\n",
    "    \n",
    "    if from_label and to_label:\n",
    "        combined_query = f\"{from_label} + {to_label}\"\n",
    "        print(f\"Searching papers for: {combined_query}\")\n",
    "        print('----------------------------------------------------------')\n",
    "        papers = search_papers(combined_query)\n",
    "        print(f\"Results for {combined_query}: {papers}\")\n",
    "        print('===================================================================================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91741dc3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
